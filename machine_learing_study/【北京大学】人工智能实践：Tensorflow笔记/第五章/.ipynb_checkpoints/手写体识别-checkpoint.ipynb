{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training step[s],loss on training batch is 2.86988.\n",
      "After 1001 training step[s],loss on training batch is 0.411361.\n",
      "After 2001 training step[s],loss on training batch is 0.285493.\n",
      "After 3001 training step[s],loss on training batch is 0.25505.\n",
      "After 4001 training step[s],loss on training batch is 0.273692.\n",
      "After 5001 training step[s],loss on training batch is 0.195581.\n",
      "After 6001 training step[s],loss on training batch is 0.242139.\n",
      "After 7001 training step[s],loss on training batch is 0.20572.\n",
      "After 8001 training step[s],loss on training batch is 0.209449.\n",
      "After 9001 training step[s],loss on training batch is 0.173513.\n",
      "After 10001 training step[s],loss on training batch is 0.170719.\n",
      "After 11001 training step[s],loss on training batch is 0.188477.\n",
      "After 12001 training step[s],loss on training batch is 0.183275.\n",
      "After 13001 training step[s],loss on training batch is 0.16074.\n",
      "After 14001 training step[s],loss on training batch is 0.162774.\n",
      "After 15001 training step[s],loss on training batch is 0.172896.\n",
      "After 16001 training step[s],loss on training batch is 0.157608.\n",
      "After 17001 training step[s],loss on training batch is 0.165898.\n",
      "After 18001 training step[s],loss on training batch is 0.156674.\n",
      "After 19001 training step[s],loss on training batch is 0.15585.\n",
      "After 20001 training step[s],loss on training batch is 0.142206.\n",
      "After 21001 training step[s],loss on training batch is 0.144018.\n",
      "After 22001 training step[s],loss on training batch is 0.137335.\n",
      "After 23001 training step[s],loss on training batch is 0.145095.\n",
      "After 24001 training step[s],loss on training batch is 0.14799.\n",
      "After 25001 training step[s],loss on training batch is 0.138227.\n",
      "After 26001 training step[s],loss on training batch is 0.142037.\n",
      "After 27001 training step[s],loss on training batch is 0.146529.\n",
      "After 28001 training step[s],loss on training batch is 0.140003.\n",
      "After 29001 training step[s],loss on training batch is 0.138203.\n",
      "After 30001 training step[s],loss on training batch is 0.134443.\n",
      "After 31001 training step[s],loss on training batch is 0.143409.\n",
      "After 32001 training step[s],loss on training batch is 0.139243.\n",
      "After 33001 training step[s],loss on training batch is 0.1307.\n",
      "After 34001 training step[s],loss on training batch is 0.135487.\n",
      "After 35001 training step[s],loss on training batch is 0.130444.\n",
      "After 36001 training step[s],loss on training batch is 0.133754.\n",
      "After 37001 training step[s],loss on training batch is 0.131861.\n",
      "After 38001 training step[s],loss on training batch is 0.130786.\n",
      "After 39001 training step[s],loss on training batch is 0.134804.\n",
      "After 40001 training step[s],loss on training batch is 0.132155.\n",
      "After 41001 training step[s],loss on training batch is 0.124787.\n",
      "After 42001 training step[s],loss on training batch is 0.133814.\n",
      "After 43001 training step[s],loss on training batch is 0.126494.\n",
      "After 44001 training step[s],loss on training batch is 0.126118.\n",
      "After 45001 training step[s],loss on training batch is 0.12893.\n",
      "After 46001 training step[s],loss on training batch is 0.123152.\n",
      "After 47001 training step[s],loss on training batch is 0.131018.\n",
      "After 48001 training step[s],loss on training batch is 0.130347.\n",
      "After 49001 training step[s],loss on training batch is 0.13367.\n",
      "After 50001 training step[s],loss on training batch is 0.125283.\n"
     ]
    }
   ],
   "source": [
    "# mnist_forward.py\n",
    "# 前项网络层搭建\n",
    "import tensorflow as tf\n",
    "\n",
    "#输入维度  输出维度 中间节点设置\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "# 权重层设置\n",
    "def get_weight(shape,regularizer):\n",
    "    w=tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    if regularizer !=None : \n",
    "        tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "# 偏置参数设置\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return b\n",
    "\n",
    "# 前向网络层\n",
    "def forward(x,regularizer):\n",
    "    # 第一层  权重层 输入维度 中间维度 正则化\n",
    "    w1=get_weight([INPUT_NODE,LAYER1_NODE],regularizer)\n",
    "    # 偏置参数 中间维度\n",
    "    b1=get_bias([LAYER1_NODE])\n",
    "    # relu激活函数 对输出结果\n",
    "    y1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "    \n",
    "    # 第二层 中间维度 输出维度 正则化\n",
    "    w2=get_weight([LAYER1_NODE,OUTPUT_NODE],regularizer)\n",
    "    # 偏置参数 输出维度\n",
    "    b2=get_bias([OUTPUT_NODE])\n",
    "    # 输出结果\n",
    "    y=tf.matmul(y1,w2)+b2\n",
    "    # 返回最后结果\n",
    "    return y\n",
    "\n",
    "# mnist_backward.py\n",
    "# 定义反向梯度求导层\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 200  #每一步训练的样本个数\n",
    "LEARNING_RATE_BASE = 0.1  # 最开始学习率\n",
    "LEARNING_RATE_DECAY = 0.99 # 学习率衰减率\n",
    "REGULARIZER = 0.0001 # 正则化参数\n",
    "STEPS = 50001  #总共训练步数\n",
    "MOVING_AVERAGE_DECAY = 0.99 # 滑动平均衰减率\n",
    "MODEL_SAVE_PATH = \"./model/\" # 模型保存地址\n",
    "MODEL_NAME=\"mnist_model\" # 模型保存名字\n",
    "\n",
    "def backward(mnist):\n",
    "    # x y_ 占位\n",
    "    x=tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "    y_=tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "    # 通过前项传播程序计算y\n",
    "    y=forward(x,REGULARIZER)\n",
    "    # 轮数计算器 设置为不可训练\n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    \n",
    "    # 计算损失函数 通过交叉熵 + 正则化\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "    cem = tf.reduce_mean(ce)\n",
    "    loss = cem+tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    # 计算指数衰减学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,mnist.train.num_examples/BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    \n",
    "    # 定义训练过程\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    # 定义滑动平均\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    ema_op=ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step,ema_op]):\n",
    "        train_op=tf.no_op(name='train')\n",
    "    \n",
    "    #保存\n",
    "    saver=tf.train.Saver()\n",
    "    \n",
    "    # 开始训练\n",
    "    with tf.Session() as sess:\n",
    "        # 初始化参数\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        # 断点继续\n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "\n",
    "        for i in range(STEPS):\n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            _,loss_value,step = sess.run([train_op,loss,global_step],feed_dict={x:xs,y_:ys})\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(\"After %d training step[s],loss on training batch is %g.\"%(step,loss_value))\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=global_step)\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "backward(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-6bd9cdac1d96>:104: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\sha\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\sha\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\sha\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\sha\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\sha\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-50001\n",
      "After 51001 training step[s],loss on training batch is 0.130511.\n",
      "After 52001 training step[s],loss on training batch is 0.126263.\n",
      "After 53001 training step[s],loss on training batch is 0.129688.\n",
      "After 54001 training step[s],loss on training batch is 0.122743.\n",
      "After 55001 training step[s],loss on training batch is 0.128493.\n"
     ]
    }
   ],
   "source": [
    "# 这段代码不能和上面的代码同时运行，否则会因为有参数重复而出错，我也不知道为什么，谜一样的\n",
    "# mnist_forward.py\n",
    "# 前项网络层搭建\n",
    "import tensorflow as tf\n",
    "\n",
    "#输入维度  输出维度 中间节点设置\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "# 权重层设置\n",
    "def get_weight(shape,regularizer):\n",
    "    w=tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    if regularizer !=None : \n",
    "        tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "# 偏置参数设置\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return b\n",
    "\n",
    "# 前向网络层\n",
    "def forward(x,regularizer):\n",
    "    # 第一层  权重层 输入维度 中间维度 正则化\n",
    "    w1=get_weight([INPUT_NODE,LAYER1_NODE],regularizer)\n",
    "    # 偏置参数 中间维度\n",
    "    b1=get_bias([LAYER1_NODE])\n",
    "    # relu激活函数 对输出结果\n",
    "    y1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "    \n",
    "    # 第二层 中间维度 输出维度 正则化\n",
    "    w2=get_weight([LAYER1_NODE,OUTPUT_NODE],regularizer)\n",
    "    # 偏置参数 输出维度\n",
    "    b2=get_bias([OUTPUT_NODE])\n",
    "    # 输出结果\n",
    "    y=tf.matmul(y1,w2)+b2\n",
    "    # 返回最后结果\n",
    "    return y\n",
    "\n",
    "# mnist_backward.py\n",
    "# 定义反向梯度求导层\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 200  #每一步训练的样本个数\n",
    "LEARNING_RATE_BASE = 0.1  # 最开始学习率\n",
    "LEARNING_RATE_DECAY = 0.99 # 学习率衰减率\n",
    "REGULARIZER = 0.0001 # 正则化参数\n",
    "STEPS = 50001  #总共训练步数\n",
    "MOVING_AVERAGE_DECAY = 0.99 # 滑动平均衰减率\n",
    "MODEL_SAVE_PATH = \"./model/\" # 模型保存地址\n",
    "MODEL_NAME=\"mnist_model\" # 模型保存名字\n",
    "\n",
    "def backward(mnist):\n",
    "    # x y_ 占位\n",
    "    x=tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "    y_=tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "    # 通过前项传播程序计算y\n",
    "    y=forward(x,REGULARIZER)\n",
    "    # 轮数计算器 设置为不可训练\n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    \n",
    "    # 计算损失函数 通过交叉熵 + 正则化\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "    cem = tf.reduce_mean(ce)\n",
    "    loss = cem+tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    # 计算指数衰减学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,mnist.train.num_examples/BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    \n",
    "    # 定义训练过程\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    # 定义滑动平均\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    ema_op=ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step,ema_op]):\n",
    "        train_op=tf.no_op(name='train')\n",
    "    \n",
    "    #保存\n",
    "    saver=tf.train.Saver()\n",
    "    \n",
    "    # 开始训练\n",
    "    with tf.Session() as sess:\n",
    "        # 初始化参数\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        # 断点继续\n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "\n",
    "        for i in range(STEPS,STEPS+5000):\n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            _,loss_value,step = sess.run([train_op,loss,global_step],feed_dict={x:xs,y_:ys})\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(\"After %d training step[s],loss on training batch is %g.\"%(step,loss_value))\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=global_step)\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "backward(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-50001\n",
      "After mnist_model-50001 traing step(s),test accuracy=0.9802\n"
     ]
    }
   ],
   "source": [
    "# mnist_test.py\n",
    "# 进行测试\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "TEST_INTERVAL_SECS = 5\n",
    "\n",
    "def test(mnist):\n",
    "    #复现计算图\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x=tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "        y_=tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "        y=forward(x,None)\n",
    "\n",
    "        ema=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        ema_restore = ema.variables_to_restore()\n",
    "        saver=tf.train.Saver(ema_restore)\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "        accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        \n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "                global_step=ckpt.model_checkpoint_path.split('/')[-1].split('.')[-1]\n",
    "                accuracy_score=sess.run(accuracy,feed_dict={x:mnist.test.images,y_:mnist.test.labels})\n",
    "                print('After %s traing step(s),test accuracy=%g'%(global_step,accuracy_score))\n",
    "            else:\n",
    "                print('No checkpoint file found')\n",
    "                return\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "test(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √实现“Mnist 数据集手写数字识别”的常用函数：\n",
    "①tf.get_collection(“”)函数表示从 collection 集合中取出全部变量生成<br>\n",
    "一个列表。<br>\n",
    "②tf.add( )函数表示将参数列表中对应元素相加。<br>\n",
    "例如：<br>\n",
    "x=tf.constant([[1,2],[1,2]])<br>\n",
    "y=tf.constant([[1,1],[1,2]])<br>\n",
    "z=tf.add(x,y)<br>\n",
    "print z<br>\n",
    "输出结果：[[2,3],[2,4]]<br>\n",
    "③tf.cast(x,dtype)函数表示将参数 x 转换为指定数据类型。<br>\n",
    "例如：<br>\n",
    "A = tf.convert_to_tensor(np.array([[1,1,2,4], [3,4,8,5]]))<br>\n",
    "print A.dtype<br>\n",
    "b = tf.cast(A, tf.float32)<br>\n",
    "print b.dtype<br>\n",
    "结果输出：<br>\n",
    "<dtype: 'int64'><br>\n",
    "<dtype: 'float32'><br>\n",
    "从输出结果看出，将矩阵 A 由整数型变为 32 位浮点型。<br>\n",
    "④tf.equal( )函数表示对比两个矩阵或者向量的元素。若对应元素相等，则返<br>\n",
    "回 True；若对应元素不相等，则返回 False。<br>\n",
    "例如：<br>\n",
    "A = [[1,3,4,5,6]]<br>\n",
    "B = [[1,3,4,3,2]]<br>\n",
    "with tf.Session( ) as sess:<br>\n",
    "print(sess.run(tf.equal(A, B)))<br>\n",
    "输出结果：[[ True True True False False]]<br>\n",
    "在矩阵 A 和 B 中，第 1、2、3 个元素相等，第 4、5 个元素不等，故输出结果中，<br>\n",
    "第 1、2、3 个元素取值为 True，第 4、5 个元素取值为 False。<br>\n",
    "⑤tf.reduce_mean(x,axis)函数表示求取矩阵或张量指定维度的平均值。若不<br>\n",
    "指定第二个参数，则在所有元素中取平均值；若指定第二个参数为 0，则在第一<br>\n",
    "维元素上取平均值，即每一列求平均值；若指定第二个参数为 1，则在第二维元<br>\n",
    "素上取平均值，即每一行求平均值。<br>\n",
    "例如：<br>\n",
    "x = [[1., 1.]<br>\n",
    "[2., 2.]]<br>\n",
    "print(tf.reduce_mean(x))<br>\n",
    "输出结果：1.5<br>\n",
    "print(tf.reduce_mean(x, 0))<br>\n",
    "输出结果：[1.5, 1.5]<br>\n",
    "print(tf.reduce_mean(x, 1))<br>\n",
    "输出结果：[1., 1.]<br>\n",
    "⑥tf.argmax(x,axis)函数表示返回指定维度 axis 下，参数 x 中最大值索引号。<br>\n",
    "例如：<br>\n",
    "在 tf.argmax([1,0,0],1)函数中，axis 为 1，参数 x 为[1,0,0]，表示在参数 x<br>\n",
    "的第一个维度取最大值对应的索引号，故返回 0。<br>\n",
    "⑦os.path.join()函数表示把参数字符串按照路径命名规则拼接。 <br>\n",
    "例如：<br>\n",
    "import os<br>\n",
    "os.path.join('/hello/','good/boy/','doiido')<br>\n",
    "输出结果：'/hello/good/boy/doiido'<br>\n",
    "⑧字符串.split( )函数表示按照指定“拆分符”对字符串拆分，返回拆分列表。<br>\n",
    "例如：<br>\n",
    "'./model/mnist_model-1001'.split('/')[-1].split('-')[-1]<br>\n",
    "在该例子中，共进行两次拆分。第一个拆分符为‘/’，返回拆分列表，并提取<br>\n",
    "列表中索引为-1 的元素即倒数第一个元素；第二个拆分符为‘-’，返回拆分列<br>\n",
    "表，并提取列表中索引为-1 的元素即倒数第一个元素，故函数返回值为 1001。<br>\n",
    "⑨tf.Graph( ).as_default( )函数表示将当前图设置成为默认图，并返回一<br>\n",
    "个上下文管理器。该函数一般与 with 关键字搭配使用，应用于将已经定义好<br>\n",
    "的神经网络在计算图中复现。<br>\n",
    "例如：<br>\n",
    "with tf.Graph().as_default() as g，表示将在 Graph()内定义的节点加入到<br>\n",
    "计算图 g 中。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √神经网络模型的保存                                                                   \n",
    "在反向传播过程中，一般会间隔一定轮数保存一次神经网络模型，并产生三个                     <br>\n",
    "文件（保存当前图结构的.meta 文件、保存当前参数名的.index 文件、保存当                    <br>\n",
    "前参数的.data 文件），在 Tensorflow 中如下表示：                                         <br>\n",
    "```python\n",
    "saver = tf.train.Saver()                                                                \n",
    "with tf.Session() as sess:                                                               \n",
    "    for i in range(STEPS):                                                                  \n",
    "        if i % 轮数 == 0:                                                                       \n",
    "            saver.save(sess, os.path.join(MODEL_SAVE_PATH,                                        \n",
    "                     MODEL_NAME), global_step=global_step)                                              \n",
    "```\n",
    "其中，tf.train.Saver()用来实例化 saver 对象。上述代码表示，神经网络每循                  <br>\n",
    "环规定的轮数，将神经网络模型中所有的参数等信息保存到指定的路径中，并在                   <br>\n",
    "存放网络模型的文件夹名称中注明保存模型时的训练轮数。                                     <br>\n",
    "√神经网络模型的加载                                                                      <br>\n",
    "在测试网络效果时，需要将训练好的神经网络模型加载，在 Tensorflow 中这                     <br>\n",
    "样表示：\n",
    "```python\n",
    "with tf.Session() as sess:                                                               \n",
    "    ckpt = tf.train.get_checkpoint_state(存储路径)                                          \n",
    "    if ckpt and ckpt.model_checkpoint_path:                                                 \n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)  \n",
    "```\n",
    "在 with 结构中进行加载保存的神经网络模型，若 ckpt 和保存的模型在指定路                   <br>\n",
    "径中存在，则将保存的神经网络模型加载到当前会话中。                                       <br>\n",
    "√加载模型中参数的滑动平均值                                                              <br>\n",
    "在保存模型时，若模型中采用滑动平均，则参数的滑动平均值会保存在相应文件                   <br>\n",
    "中。通过实例化 saver 对象，实现参数滑动平均值的加载，在 Tensorflow 中如                  <br>\n",
    "下表示：                                                                                 <br>\n",
    "ema = tf.train.ExponentialMovingAverage(滑动平均基数)                                    <br>\n",
    "ema_restore = ema.variables_to_restore()                                                 <br>\n",
    "saver = tf.train.Saver(ema_restore)                                                      <br>\n",
    "√神经网络模型准确率评估方法                                                              <br>\n",
    "在网络评估时，一般通过计算在一组数据上的识别准确率，评估神经网络的效                     <br>\n",
    "果。在 Tensorflow 中这样表示：                                                           <br>\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))                         <br>\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                       <br>\n",
    "在上述中，y 表示在一组数据（即 batch_size 个数据）上神经网络模型的预测                   <br>\n",
    "结果，y 的形状为[batch_size,10]，每一行表示一张图片的识别结果。通过                      <br>\n",
    "tf.argmax()函数取出每张图片对应向量中最大值元素对应的索引值，组成长度                    <br>\n",
    "为输入数据 batch_size 个的一维数组。通过 tf.equal()函数判断预测结果张量                  <br>\n",
    "和实际标签张量的每个维度是否相等，若相等则返回 True，不相等则返回 False。                <br>\n",
    "通 过 tf.cast() 函数将 得到的 布 尔 型 数 值 转 化 为 实 数 型 ， 再通过                 <br>\n",
    "tf.reduce_mean()函数求平均值，最终得到神经网络模型在本组数据上的准确率。                 <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
