{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow-1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "\n",
    "start_token = 'G'\n",
    "end_token = 'E'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_poems(file_name):\n",
    "    poems = []\n",
    "    with open(file_name, \"r\", encoding='utf-8', ) as f:\n",
    "        for line in f.readlines():\n",
    "            try:\n",
    "                title, content = line.strip().split(':')\n",
    "                content = content.replace(' ', '')\n",
    "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content or \\\n",
    "                                start_token in content or end_token in content:\n",
    "                    continue\n",
    "                if len(content) < 5 or len(content) > 80:\n",
    "                    continue\n",
    "                content = start_token + content + end_token\n",
    "                poems.append(content)\n",
    "            except ValueError as e:\n",
    "                pass\n",
    "    # 按诗的字数排序\n",
    "    poems = sorted(poems, key=lambda line: len(line))\n",
    "    # 统计每个字出现次数\n",
    "    all_words = []\n",
    "    for poem in poems:\n",
    "        all_words += [word for word in poem]  \n",
    "    counter = collections.Counter(all_words)  # 统计词和词频。\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*count_pairs)\n",
    "    words = words[:len(words)] + (' ',)\n",
    "    word_int_map = dict(zip(words, range(len(words))))\n",
    "    poems_vector = [list(map(word_int_map.get, poem)) for poem in poems]\n",
    "    return poems_vector, word_int_map, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems_vector, word_int_map, words = process_poems('../poems.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=\\\n",
    "[33, 289, 13, 58, 44, 0, 13, 23, 36, 892, 134, 1, 13, 5, 9, 19, 192, 0, 4, 34, 4, 26, 134, 1, 4, 85, 6, 431, 18, 0, 13, 79, 10, 45, 78, 1, 15, 129, 4, 15, 42, 0, 4, 63, 4, 26, 24, 1, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江寂春门里，春年白史衣。春人无有意，不夜不相衣。不色山湘上，春开一客深。何书不何处，不路不相归。EE\n"
     ]
    }
   ],
   "source": [
    "mystr=''\n",
    "for w in word:\n",
    "    mystr+=words[w]\n",
    "print(mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rnn_lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(model, input_data, output_data, vocab_size, rnn_size=128, num_layers=2, batch_size=64,\n",
    "              learning_rate=0.01):\n",
    "    end_points = {}\n",
    "    # 构建RNN基本单元RNNcell\n",
    "    if model == 'rnn':\n",
    "        cell_fun = tf.contrib.rnn.BasicRNNCell\n",
    "    elif model == 'gru':\n",
    "        cell_fun = tf.contrib.rnn.GRUCell\n",
    "    else:\n",
    "        cell_fun = tf.contrib.rnn.BasicLSTMCell\n",
    "    #？？？？？？？？？？？？？？？？？？？？？？\n",
    "    # 每层128个小单元，一共有两层，输出的Ct 和 Ht 要分开放到两个tuple中\n",
    "    # 在下面补全代码 \n",
    "    #################################################\n",
    "    cells = [cell_fun(num_units=rnn_size) for n in range(num_layers)]\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    #################################################\n",
    "    # 如果是训练模式，output_data不为None，则初始状态shape为[batch_size * rnn_size]\n",
    "    # 如果是生成模式，output_data为None，则初始状态shape为[1 * rnn_size]\n",
    "    if output_data is not None:\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    else:\n",
    "        initial_state = cell.zero_state(1, tf.float32)\n",
    "\n",
    "    # 构建隐层\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.Variable(tf.random_uniform([vocab_size + 1, rnn_size], -1.0, 1.0),name = 'embedding')\n",
    "        inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    #？？？？？？？？？？？？？？？？？？？？？？？？？？\n",
    "    ####################################################    \n",
    "    outputs, last_state = tf.nn.dynamic_rnn(cell,inputs,initial_state=initial_state,dtype=tf.float32)# 填写里面的内容\n",
    "    ######################################################\n",
    "    output = tf.reshape(outputs, [-1, rnn_size])\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([rnn_size, vocab_size + 1]))\n",
    "    bias = tf.Variable(tf.zeros(shape=[vocab_size + 1]))\n",
    "    logits = tf.nn.bias_add(tf.matmul(output, weights), bias=bias) # 一层全连接\n",
    "\n",
    "\n",
    "    if output_data is not None: # 训练模式\n",
    "        labels = tf.one_hot(tf.reshape(output_data, [-1]), depth=vocab_size + 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)  # 优化器用的 adam\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['output'] = output\n",
    "        end_points['train_op'] = train_op\n",
    "        end_points['total_loss'] = total_loss\n",
    "        end_points['loss'] = loss\n",
    "        end_points['last_state'] = last_state\n",
    "    else: # 生成模式\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['last_state'] = last_state\n",
    "        end_points['prediction'] = prediction\n",
    "    return end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    # 处理数据集\n",
    "    poems_vector, word_to_int, vocabularies = process_poems('../poems.txt')\n",
    "    # 生成batch\n",
    "    batches_inputs, batches_outputs = generate_batch(64, poems_vector, word_to_int)\n",
    "\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    output_targets = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    # 构建模型\n",
    "    end_points = rnn_model(model='lstm', input_data=input_data, output_data=output_targets, vocab_size=len(\n",
    "        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=0.01)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "#     int now=0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        for epoch in range(50):\n",
    "            n = 0\n",
    "            n_chunk = len(poems_vector) // batch_size\n",
    "            for batch in range(n_chunk):\n",
    "                loss, _, _ = sess.run([\n",
    "                    end_points['total_loss'],\n",
    "                    end_points['last_state'],\n",
    "                    end_points['train_op']\n",
    "                ], feed_dict={input_data: batches_inputs[n], output_targets: batches_outputs[n]})\n",
    "                n += 1\n",
    "                print('\\r[INFO] Epoch: %d , batch: %d , training loss: %.6f' % (epoch, batch, loss),end=\" \")\n",
    "        saver.save(sess, './poem_generator')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成 诗歌部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_poem(begin_word):\n",
    "    batch_size = 1\n",
    "    poems_vector, word_int_map, vocabularies = process_poems('../poems.txt')\n",
    "\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "\n",
    "    end_points = rnn_model(model='lstm', input_data=input_data, output_data=None, vocab_size=len(\n",
    "        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=0.01)\n",
    "    # 如果指定开始的字\n",
    "    if begin_word:\n",
    "        word = begin_word\n",
    "    else:\n",
    "        word = to_word(predict, vocabularies)\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        saver.restore(sess, './poem_generator')  # 恢复之前训练好的模型\n",
    "        poem = ''\n",
    "        # ???????????????????????????????????????\n",
    "        # 下面部分代码主要功能是根据指定的开始字符来生成诗歌\n",
    "        #########################################\n",
    "        poem+=(word)\n",
    "        while (poem[-1] != end_token and len(poem) < 100):\n",
    "            initial_state, last_state, prediction = sess.run([\n",
    "                end_points['initial_state'],\n",
    "                end_points['last_state'],\n",
    "                end_points['prediction']\n",
    "            ], feed_dict={input_data: [[word_int_map[poem[-1]]]]})\n",
    "            poem+=vocabularies[np.argmax(prediction)]\n",
    "        #########################################\n",
    "        return poem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他的一些处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, poems_vec, word_to_int):\n",
    "    # 每次取64首诗进行训练\n",
    "    n_chunk = len(poems_vec) // batch_size\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "\n",
    "        batches = poems_vec[start_index:end_index]\n",
    "        # 找到这个batch的所有poem中最长的poem的长度\n",
    "        length = max(map(len, batches))\n",
    "        # 填充一个这么大小的空batch，空的地方放空格对应的index标号\n",
    "        x_data = np.full((batch_size, length), word_to_int[' '], np.int32)\n",
    "        for row in range(batch_size):\n",
    "            x_data[row, :len(batches[row])] = batches[row]\n",
    "        y_data = np.copy(x_data)\n",
    "        y_data[:, :-1] = x_data[:, 1:]\n",
    "        \"\"\"\n",
    "        x_data             y_data\n",
    "        [6,2,4,6,9]       [2,4,6,9,9]\n",
    "        [1,4,2,8,5]       [4,2,8,5,5]\n",
    "        \"\"\"\n",
    "        x_batches.append(x_data)\n",
    "        y_batches.append(y_data)\n",
    "    return x_batches, y_batches\n",
    "\n",
    "def to_word(predict, vocabs):# 预测的结果转化成汉字\n",
    "    sample = np.argmax(predict)\n",
    "    if sample > len(vocabs):\n",
    "        sample = len(vocabs) - 1\n",
    "    return vocabs[sample]\n",
    "def pretty_print_poem(poem):#  令打印的结果更工整\n",
    "    poem_sentences = poem.split('。')\n",
    "    for s in poem_sentences:\n",
    "        if s != '' and len(s) > 10:\n",
    "            print(s + '。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train tang poem...\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-91011f1fa68b>:15: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-91011f1fa68b>:16: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From c:\\users\\sha\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-91011f1fa68b>:31: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From <ipython-input-5-91011f1fa68b>:42: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\sha\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[INFO] Epoch: 49 , batch: 542 , training loss: 5.802704                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              "
     ]
    }
   ],
   "source": [
    "print('[INFO] train tang poem...')\n",
    "run_training() # 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] write tang poem...\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-7-91011f1fa68b>:15: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-7-91011f1fa68b>:16: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From c:\\users\\sha\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-7-91011f1fa68b>:31: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\users\\sha\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./poem_generator\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] write tang poem...')\n",
    "poem2 = gen_poem('月')# 生成诗歌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "月明日暮春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳春风景阳。\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 25)\n",
    "pretty_print_poem(poem2)\n",
    "print('#' * 25)\n",
    "#训练模型时间比较长，训练模型完成后每次生成诗歌的时，不需要再次训练 ，可以注销上面的 run_training()。生成部分执行速度很快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
